{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e484784-f3fe-4d70-a56d-8684c046a62a",
   "metadata": {},
   "source": [
    "#### Libraries used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e51f7d6-344f-4a49-beee-06ada7b10178",
   "metadata": {},
   "source": [
    " Requests & BeautifulSoup: Used for fetching and parsing HTML pages.\n",
    "     \n",
    " CSV & JSON: For saving scraped data.\n",
    "\n",
    " Logging: Logs important messages and errors.\n",
    "     \n",
    " Threading and Time: Useful for handling multiple requests and measuring execution time.\n",
    "\n",
    " Selenium and WebDriver: Used for handling JavaScript-rendered pages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d98cf7-ca2b-4c04-8807-8eff11697e0c",
   "metadata": {},
   "source": [
    "#### Logging:\n",
    "\n",
    "Sets up logging to a file named 'scraper.log'\n",
    "\n",
    "Logs INFO level messages and above.\n",
    "\n",
    "It includes timestamp, log level, and message\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "11539cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import json\n",
    "import logging\n",
    "import threading\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "\n",
    "#Configures logging to record events in a file (scraper.log).\n",
    "logging.basicConfig(filename='scraper.log', level=logging.INFO, \n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f714eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8d4a5537",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_page(url, use_selenium=False):\n",
    "    \"\"\"\n",
    "    Fetch page content using either requests or Selenium\n",
    "    Requests work if page is static while Selenium works if page loads dynamically using javascript\n",
    "    \n",
    "    \"\"\"\n",
    "    if use_selenium:\n",
    "        return fetch_with_selenium(url)\n",
    "    else:\n",
    "        headers = {'User-Agent': 'Mozilla/5.0'}  #headers are used to mimic real browsers\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            return response.text\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logging.error(f\"Failed to fetch {url}: {e}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca1a5ba-f2b5-42dd-8449-af6b29d63b61",
   "metadata": {},
   "source": [
    "##### Main function to fetch page content.\n",
    "##### Uses regular requests by default and switches to Selenium if needed (for JavaScript content).\n",
    "##### Requests work if page is static while Selenium works if page loads dynamically using javascript\n",
    "##### Uses a user-agent header to mimic a real browser.\n",
    "##### Implements error handling to log failures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "439035ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_with_selenium(url):\n",
    "    \"\"\"Use Selenium to get JavaScript-rendered content\"\"\"\n",
    "    try:\n",
    "        options = Options()\n",
    "        options.add_argument(\"--headless\")  # Run in headless mode\n",
    "        options.add_argument(\"--disable-gpu\")\n",
    "        options.add_argument(\"--window-size=1920,1080\")\n",
    "        \n",
    "        driver = webdriver.Chrome(options=options)\n",
    "        driver.get(url)\n",
    "        \n",
    "        # Wait for articles to load\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, \"[data-testid='edinburgh-card']\"))\n",
    "        )\n",
    "        \n",
    "        html = driver.page_source\n",
    "        driver.quit()\n",
    "        return html\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Selenium error: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e65727-7503-465d-94ae-e167f02da4fd",
   "metadata": {},
   "source": [
    "##### Uses Selenium WebDriver to fetch pages requiring JavaScript rendering.\n",
    "##### Runs in headless mode (without opening a browser window).\n",
    "##### Waits until specific elements ([data-testid='edinburgh-card']) are loaded.\n",
    "##### The driver opens the webpage and waits until news articles appear before extracting content.\n",
    "##### Returns page HTML source.\n",
    "##### Properly closes the browser and handles errors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "30002227",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracts title, summary, date, and link from the HTML content.\n",
    "#Uses BeautifulSoup to find elements with specific data attributes.\n",
    "\n",
    "def parse_articles(html):\n",
    "    \"\"\"Parse BBC News articles from HTML\"\"\"\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    articles = []\n",
    "    \n",
    "    # Modern BBC News selectors (as of 2025)\n",
    "    for article in soup.select(\"[data-testid='edinburgh-card'], [data-testid='london-card']\"):\n",
    "        try:\n",
    "            title_elem = article.select_one(\"h2[data-testid='card-headline']\")\n",
    "            link_elem = article.select_one(\"a[data-testid='internal-link']\")\n",
    "            summary_elem = article.select_one(\"p[data-testid='card-description']\")\n",
    "            time_elem = article.select_one(\"time\")\n",
    "            \n",
    "            if not title_elem or not link_elem:\n",
    "                continue\n",
    "                \n",
    "            title = title_elem.get_text(strip=True)\n",
    "            link = link_elem['href']\n",
    "            \n",
    "            if not link.startswith('http'):\n",
    "                link = f'https://www.bbc.com{link}'\n",
    "                \n",
    "            summary = summary_elem.get_text(strip=True) if summary_elem else ''\n",
    "            date = time_elem['datetime'] if time_elem and time_elem.has_attr('datetime') else ''\n",
    "            \n",
    "            articles.append({\n",
    "                'title': title,\n",
    "                'date': date,\n",
    "                'summary': summary,\n",
    "                'link': link\n",
    "            })\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error parsing article: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return articles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d373d641-c736-485e-a196-9a73a3f27abe",
   "metadata": {},
   "source": [
    "##### Parses HTML using BeautifulSoup.\n",
    "##### Finds article elements using CSS selectors.\n",
    "##### Handles missing elements.\n",
    "##### Logs parsing errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466504c0-ceed-4230-b511-92110fbd75ec",
   "metadata": {},
   "source": [
    "##### Extracts:\n",
    "\n",
    "Title\n",
    "\n",
    "Link (converts to absolute URL if relative)\n",
    "\n",
    "Summary (if available)\n",
    "\n",
    "Publication date (if available)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ed00751e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pagination handling\n",
    "#Looks for the \"Next\" button in the page navigation.Extracts the next pageâ€™s URL to continue scraping.\n",
    "\n",
    "def get_next_page_url(soup, current_url):\n",
    "    \"\"\"Find the next page URL for BBC News\"\"\"\n",
    "    \n",
    "    #Constructs the full URL if the link is relative.\n",
    "    next_button = soup.select_one('a[aria-label=\"Next\"]')\n",
    "    if next_button and 'href' in next_button.attrs:\n",
    "        next_url = next_button['href']\n",
    "        if not next_url.startswith('http'):\n",
    "            base_url = current_url.split('/news')[0]\n",
    "            next_url = f\"{base_url}{next_url}\"\n",
    "        return next_url\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96234b1d-0b71-4670-b92f-1fd8f94f73a7",
   "metadata": {},
   "source": [
    "##### Finds the \"Next\" page button in the pagination.\n",
    "##### Constructs absolute URL if the href is relative.\n",
    "##### Returns None if no next page exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7c972838",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loops through multiple pages to collect articles.\n",
    "#Tries requests first, then switches to Selenium if no articles are found.\n",
    "\n",
    "def scrape_news(base_url, max_pages=5):\n",
    "    \"\"\"Scrape BBC News with proper pagination handling\"\"\"\n",
    "    all_articles = []\n",
    "    current_url = base_url\n",
    "    pages_scraped = 0\n",
    "    \n",
    "    while current_url and pages_scraped < max_pages:\n",
    "        logging.info(f\"Scraping {current_url}\")\n",
    "        \n",
    "        #Extracts and appends articles until reaching max_pages.\n",
    "        # First try with requests\n",
    "        html = fetch_page(current_url)\n",
    "        \n",
    "        # If no articles found, try with Selenium\n",
    "        if html and len(parse_articles(html)) == 0:\n",
    "            html = fetch_page(current_url, use_selenium=True)\n",
    "        \n",
    "        if html:\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            articles = parse_articles(html)\n",
    "            all_articles.extend(articles)\n",
    "            pages_scraped += 1\n",
    "            \n",
    "            # Get next page URL\n",
    "            current_url = get_next_page_url(soup, current_url)\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    return all_articles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36bd7a7-c658-4826-ab4b-7d0201db5e24",
   "metadata": {},
   "source": [
    "##### Main scraping logic.\n",
    "##### Handles pagination up to max_pages.\n",
    "##### First tries regular requests, falls back to Selenium if no articles found.\n",
    "##### Collects all articles in a list.\n",
    "##### Logs progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b37ec7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saves scraped data as CSV or JSON.\n",
    "#Writes column headers for CSV.\n",
    "\n",
    "def save_data(data, filename, format='csv'):\n",
    "    \"\"\"Save data to file\"\"\"\n",
    "    if not data:\n",
    "        logging.warning(\"No data to save!\")\n",
    "        return\n",
    "     \n",
    "    #Uses UTF-8 encoding to handle special characters.\n",
    "    if format == 'csv':\n",
    "        with open(filename, 'w', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=data[0].keys())\n",
    "            writer.writeheader()\n",
    "            writer.writerows(data)\n",
    "    elif format == 'json':\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(data, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5e92a3-e659-488c-87a8-63e9122297ea",
   "metadata": {},
   "source": [
    "##### Saves data in either CSV or JSON format.\n",
    "##### Handles UTF-8 encoding properly.\n",
    "##### Creates proper headers in CSV.\n",
    "##### Pretty-prints JSON with indentation.\n",
    "##### Warns if no data to save."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "32f55e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-25 14:25:55,038 - INFO - Scraping https://www.bbc.com/news\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial request status code: 200\n",
      "Starting scraping process...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-25 14:25:55,874 - INFO - Scraping completed in 1.32 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully scraped 2 articles:\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "ARTICLE 1:\n",
      "Title: Bitcoin in the bush - the crypto mine in remote Zambia\n",
      "Summary: Bitcoin miners will go to remote locations to take advantage of cheap electricity.\n",
      "Link: https://www.bbc.com/news/articles/cly4xe373p4o\n",
      "--------------------------------------------------------------------------------\n",
      "ARTICLE 2:\n",
      "Title: Trump bemoans a portrait of him - but gets a new one from Putin\n",
      "Summary: A \"distorted\" portrait in the US was removed after the president complained, but he was \"touched\" by a Russian gift.\n",
      "Link: https://www.bbc.com/news/articles/c62xyrr20dxo\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Data saved to 'bbc_news_articles.csv' and 'bbc_news_articles.json'\n",
      "\n",
      "Scraping completed in 1.32 seconds.\n"
     ]
    }
   ],
   "source": [
    "#Main Execution Block\n",
    "\n",
    "#Measures execution time.\n",
    "if __name__ == \"__main__\":\n",
    "    start_time = time.time()\n",
    "    base_url = \"https://www.bbc.com/news\"\n",
    "    \n",
    "    # Check initial access\n",
    "    response = requests.get(base_url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "    print(f\"Initial request status code: {response.status_code}\")\n",
    "    \n",
    "    # Start scraping\n",
    "    print(\"Starting scraping process...\\n\")\n",
    "    articles = scrape_news(base_url, max_pages=2)  # Start with 2 pages\n",
    "    \n",
    "    # Print scraped articles in a readable format\n",
    "    if articles:\n",
    "        print(f\"\\nSuccessfully scraped {len(articles)} articles:\\n\")\n",
    "        print(\"-\" * 80)\n",
    "        for i, article in enumerate(articles, 1):\n",
    "            print(f\"ARTICLE {i}:\")\n",
    "            print(f\"Title: {article['title']}\")\n",
    "            if article['date']:\n",
    "                print(f\"Date: {article['date']}\")\n",
    "            if article['summary']:\n",
    "                print(f\"Summary: {article['summary']}\")\n",
    "            print(f\"Link: {article['link']}\")\n",
    "            print(\"-\" * 80)\n",
    "    else:\n",
    "        print(\"\\nNo articles were scraped. Check the log file for errors.\")\n",
    "    \n",
    "    # Save to files\n",
    "    if articles:\n",
    "        save_data(articles, \"bbc_news_articles.csv\", format='csv')\n",
    "        save_data(articles, \"bbc_news_articles.json\", format='json')\n",
    "        print(\"\\nData saved to 'bbc_news_articles.csv' and 'bbc_news_articles.json'\")\n",
    "    \n",
    "    print(f\"\\nScraping completed in {time.time() - start_time:.2f} seconds.\")\n",
    "    logging.info(f\"Scraping completed in {time.time() - start_time:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a3452f-feb1-4306-b6b2-5e0527bc2afe",
   "metadata": {},
   "source": [
    "##### Entry point when script is run directly.\n",
    "##### Times the scraping process.\n",
    "##### Prints progress and results to console.\n",
    "##### Saves data to both CSV and JSON.\n",
    "##### Provides detailed output of scraped articles.\n",
    "##### Includes performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3494b056",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f53c5ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
